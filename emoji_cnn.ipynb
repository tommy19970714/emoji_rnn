{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models               import Sequential, load_model\n",
    "from keras.layers               import Input, Dense, Activation\n",
    "from keras.layers               import LSTM, GRU, SimpleRNN\n",
    "from keras.optimizers           import RMSprop, Adam\n",
    "from keras.utils.data_utils     import get_file\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers.noise         import GaussianNoise as GN\n",
    "from keras.layers.noise         import GaussianDropout as GD\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow               as tf \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "#import msgpack\n",
    "#import msgpack_numpy as mn\n",
    "#mn.patch()\n",
    "#import MeCab\n",
    "#import plyvel\n",
    "from itertools import cycle as Cycle\n",
    "#import dill\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ðŸ’¦']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "emoji_index = pickle.loads(open('./emoji_index.pkl', 'rb').read())\n",
    "def emojiFromIndex(num):\n",
    "    return [k for k, v in emoji_index.items() if v == num]\n",
    "\n",
    "emojiFromIndex(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, num):\n",
    "    # ç²¾åº¦ã®å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    plt.plot(history.history['acc'],\"o-\",label=\"accuracy\")\n",
    "    plt.plot(history.history['val_acc'],\"o-\",label=\"val_acc\")\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # æå¤±ã®å±¥æ­´ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    plt.plot(history.history['loss'],\"o-\",label=\"loss\",)\n",
    "    plt.plot(history.history['val_loss'],\"o-\",label=\"val_loss\")\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(\"./graph/keras_graph_%d\" % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    from keras.layers import Input, Dense, Embedding, merge, Convolution2D as Conv2D, MaxPooling2D, Dropout, ZeroPadding2D\n",
    "    from keras.layers.core import Reshape, Flatten\n",
    "    from keras.models import Model, load_model\n",
    "    from keras.layers.merge import add, concatenate\n",
    "    sequence_length = 30\n",
    "    embedding_dim = 200  # 100\n",
    "    vocabulary_size = 10\n",
    "    num_filters = 512\n",
    "    filter_sizes = [3, 4, 5, 1, 2]\n",
    "    inputs = Input(shape=(sequence_length, embedding_dim,), dtype='float64')\n",
    "    #embedding = Embedding(output_dim=embedding_dim, input_dim=vocabulary_size, input_length=sequence_length)(inputs)\n",
    "    reshape = Reshape((sequence_length, embedding_dim, 1))(inputs)\n",
    "    #Conv2D(512, (3, 256), padding=\"valid\", kernel_initializer=\"normal\", data_format=\"channels_last\", activation=\"relu\")\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, (filter_sizes[0], embedding_dim),\n",
    "                    kernel_initializer=\"normal\", data_format=\"channels_last\", activation=\"relu\")(reshape)\n",
    "    conv_1 = Conv2D(num_filters, (filter_sizes[1], embedding_dim),\n",
    "                    kernel_initializer=\"normal\", data_format=\"channels_last\", activation=\"relu\")(reshape)\n",
    "    conv_2 = Conv2D(num_filters, (filter_sizes[2], embedding_dim),\n",
    "                    kernel_initializer=\"normal\", data_format=\"channels_last\", activation=\"relu\")(reshape)\n",
    "    conv_3 = Conv2D(num_filters, (filter_sizes[3], embedding_dim),\n",
    "                    kernel_initializer=\"normal\", data_format=\"channels_last\", activation=\"relu\")(reshape)\n",
    "    conv_4 = Conv2D(num_filters, (filter_sizes[4], embedding_dim),\n",
    "                    kernel_initializer=\"normal\", data_format=\"channels_last\", activation=\"relu\")(reshape)\n",
    "    # `MaxPooling2D(pool_size=(28, 1), padding=\"valid\", data_format=\"channels_last\", strides=(1, 1))\n",
    "    maxpool_0 = MaxPooling2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(\n",
    "        1, 1), padding=\"valid\", data_format=\"channels_last\")(conv_0)\n",
    "    maxpool_1 = MaxPooling2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(\n",
    "        1, 1), padding=\"valid\", data_format=\"channels_last\")(conv_1)\n",
    "    maxpool_2 = MaxPooling2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(\n",
    "        1, 1), padding=\"valid\", data_format=\"channels_last\")(conv_2)\n",
    "    maxpool_3 = MaxPooling2D(pool_size=(sequence_length - filter_sizes[3] + -3, 1), strides=(\n",
    "        1, 1), padding=\"valid\", data_format=\"channels_last\")(conv_3)\n",
    "    maxpool_4 = MaxPooling2D(pool_size=(sequence_length - filter_sizes[4] + -2, 1), strides=(\n",
    "        1, 1), padding=\"valid\", data_format=\"channels_last\")(conv_4)\n",
    "\n",
    "    merged = concatenate(\n",
    "        [maxpool_0, maxpool_1, maxpool_2, maxpool_3, maxpool_4], axis=1)\n",
    "    \n",
    "    #units = 2048\n",
    "    output = Dense(units=1, activation='sigmoid')(\n",
    "        Activation('linear')(\n",
    "            Dropout(0.5)(\n",
    "                Flatten()(merged))))\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    adam = Adam()\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_nums: [(2, 53592), (10, 38038), (9, 26034), (1, 22109), (4, 17432), (5, 16174), (7, 13489), (3, 10927), (6, 10851), (8, 9534)]\n",
      "min_num: 9534\n",
      "ã‚¯ãƒ©ã‚¹2 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 44058 (82.21ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹10 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 28504 (74.94ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹9 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 16500 (63.38ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹1 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 12575 (56.88ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹4 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 7898 (45.31ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹5 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 6640 (41.05ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹7 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 3955 (29.32ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹3 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 1393 (12.75ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹6 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 1317 (12.14ï¼…)\n",
      "ã‚¯ãƒ©ã‚¹8 å‰Šé™¤ã‚µãƒ³ãƒ—ãƒ«æ•°: 0 (0.00ï¼…)\n"
     ]
    }
   ],
   "source": [
    "dataloader = util.dataloader(dataPath='../text_vec.pkl', labelPath='../label_vec.pkl')\n",
    "dataloader.normalize()\n",
    "X_train, X_test, y_train, y_test = dataloader.dataset() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train():\n",
    "    print('Vectorization...')\n",
    "#     X = np.zeros((len(X_train), 30, 200), dtype=np.float64)\n",
    "#     y = np.zeros((len(X_train), 2048), dtype=np.float64)\n",
    "#     for i, (text, ans) in enumerate(zip(X_train, y_train)):\n",
    "#         if i%10000 == 0:\n",
    "#             print(\"building training vector... iter %d\"%i)\n",
    "#         for t, term in enumerate(text):\n",
    "#             X[i, t, :] = term\n",
    "#         y[i, :] = ans\n",
    "    model = discriminator_model()\n",
    "    for iteration in range(1, 5):\n",
    "        #for iteration in range(1, 50):\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print('Iteration', iteration)\n",
    "        history = model.fit(X_train, y_train, batch_size=128, epochs=20, validation_split=0.33)\n",
    "        MODEL_NAME = \"./models/snapshot.%09d.model\"%(iteration)\n",
    "        model.save(MODEL_NAME)\n",
    "        plot_history(history, iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred():\n",
    "    model_type = sorted(glob.glob('./models/*.model'))[-1]\n",
    "    print(\"model type is %s\"%model_type)\n",
    "    model  = load_model(model_type)\n",
    "    print(\"finished model type is %s\"%model_type)\n",
    "    #results = model.predict(np.array([X_test]))\n",
    "    y_pred = model.predict_classes(X_test, batch_size=128, verbose=1)\n",
    "    \n",
    "    target_names = [str(i) for i, e in emoji_index]\n",
    "    report = metrics.classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(report)\n",
    "    \n",
    "    matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 116944 samples, validate on 57600 samples\n",
      "Epoch 1/20\n",
      "  1152/116944 [..............................] - ETA: 53:56 - loss: -53.2457 - acc: 0.0955  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.276111). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116944/116944 [==============================] - 54s 460us/step - loss: -68.8079 - acc: 0.1010 - val_loss: -69.0535 - val_acc: 0.1011\n",
      "Epoch 2/20\n",
      "116944/116944 [==============================] - 20s 172us/step - loss: -69.1711 - acc: 0.1012 - val_loss: -69.0535 - val_acc: 0.1011\n",
      "Epoch 3/20\n",
      "116944/116944 [==============================] - 20s 171us/step - loss: -69.1711 - acc: 0.1012 - val_loss: -69.0535 - val_acc: 0.1011\n",
      "Epoch 4/20\n",
      "116944/116944 [==============================] - 20s 174us/step - loss: -69.1711 - acc: 0.1012 - val_loss: -69.0535 - val_acc: 0.1011\n",
      "Epoch 5/20\n",
      "116944/116944 [==============================] - 20s 173us/step - loss: -69.1711 - acc: 0.1012 - val_loss: -69.0535 - val_acc: 0.1011\n",
      "Epoch 6/20\n",
      "102400/116944 [=========================>....] - ETA: 2s - loss: -69.1932 - acc: 0.1014"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-57f93b2543e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-584c6bb705b8>\u001b[0m in \u001b[0;36mmain_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./models/snapshot.%09d.model\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred():\n",
    "    #m = MeCab.Tagger('-Owakati')\n",
    "    m = MeCab.Tagger('-Owakati -d /usr/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    emoji_index = pickle.loads(open('./emoji_index.pkl', 'rb').read())\n",
    "    index_emoji = {index:emoji for emoji, index in emoji_index.items()}\n",
    "  \n",
    "    print('now loading term_vec.pkl...')\n",
    "    term_vec    = pickle.loads(open('term_vec.pkl', 'rb').read())\n",
    "    print('finished loading term_vec.pkl...')\n",
    "    #model_type = sorted(glob.glob('./models/*.model'))[-1]\n",
    "    model_type = sorted(glob.glob('./models/*.model'))[-1]\n",
    "    print(\"model type is %s\"%model_type)\n",
    "    model  = load_model(model_type)\n",
    "    print(\"finished model type is %s\"%model_type)\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        print(line, end=\" \")\n",
    "        buff = [\"*\"]*30\n",
    "        for i,term in enumerate(m.parse(line).strip().split()[:30]):\n",
    "            buff[i] = term\n",
    "        X = []\n",
    "    for term in buff:\n",
    "        if term_vec.get(term) is not None:\n",
    "            X.append(term_vec[term])\n",
    "        else:\n",
    "            X.append(np.zeros(256))#100\n",
    "    results = model.predict(np.array([X]))\n",
    "    res = {index_emoji[i]:score for i,score in enumerate(results[0].tolist()[:1187])}\n",
    "    for emoji, score in sorted(filter(lambda x:x[1]>0.01, res.items()), key=lambda x:x[1]*-1)[:20]:\n",
    "        print(emoji, \"%d\"%(int(score*100)), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2323e352db4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-2748a44286c9>\u001b[0m in \u001b[0;36mpred\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#m = MeCab.Tagger('-Owakati')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-Owakati -d /usr/lib/mecab/dic/mecab-ipadic-neologd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0memoji_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./emoji_index.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindex_emoji\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0memoji\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memoji_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/MeCab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_newclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import MeCab \n",
    "pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
